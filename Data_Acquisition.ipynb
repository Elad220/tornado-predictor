{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization\n",
    "YEARS_TO_COUNT = ['1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions for creating the web driver and closing it\n",
    "def create_web_driver():\n",
    "    return webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "def start_web_driver(driver, url):\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "\n",
    "def close_web_driver(driver):\n",
    "    driver.close()\n",
    "\n",
    "def log(message,filename):\n",
    "    with open(f'{filename}', 'a') as f:\n",
    "        f.write(message + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization for NOAA\n",
    "NOAA_URL = 'https://www.ncdc.noaa.gov/stormevents/choosedates.jsp?statefips=-999,ALL'\n",
    "NOAA_BEGINMONTH_XPATH = \"//select[@id='beginDate_mm']//option[contains(text(),'\"\n",
    "NOAA_BEGINYEAR_XPATH = \"//select[@id='beginDate_yyyy']//option[contains(text(),'\"\n",
    "NOAA_BEGINDAY_XPATH = \"//select[@id='beginDate_dd']//option[contains(text(),'01')]\"\n",
    "NOAA_ENDMONTH_XPATH = \"//select[@id='endDate_mm']//option[contains(text(),'\"\n",
    "NOAA_ENDYEAR_XPATH = \"//select[@id='endDate_yyyy']//option[contains(text(),'\"\n",
    "NOAA_ENDDAY_XPATH = \"//select[@id='endDate_dd']//option[contains(text(),'\"\n",
    "NOAA_EVENTTYPE_XPATH = \"//select[@id='eventType']//option[contains(@value,'Tornado')]\"\n",
    "NOAA_SEARCH_XPATH = \"//input[@value='Search']\"\n",
    "NOAA_RETURN_XPATH = '//*[@id=\"anch_8\"]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for NOAA to parse the relevant date and input it in the relevant fields of the web driver received by input_date\n",
    "def noaa_input_start_date(driver, year, month):\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINMONTH_XPATH}{month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINMONTH_XPATH}{month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINYEAR_XPATH}{year}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINYEAR_XPATH}{year}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINDAY_XPATH}\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINDAY_XPATH}\").click()\n",
    "\n",
    "def noaa_input_end_date(driver, curr_year, end_month):\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDMONTH_XPATH}{end_month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDMONTH_XPATH}{end_month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDYEAR_XPATH}{curr_year}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDYEAR_XPATH}{curr_year}')]\").click()\n",
    "    if end_month == 2:\n",
    "        end_day = 28\n",
    "    elif end_month in [4,6,7,9,11]:\n",
    "        end_day = 30\n",
    "    else:\n",
    "        end_day = 31\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDDAY_XPATH}{end_day}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDDAY_XPATH}{end_day}')]\").click()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to click the relevant fields in NOAA web driver\n",
    "def noaa_start_search(driver):\n",
    "    driver.find_element_by_xpath(NOAA_SEARCH_XPATH).click()\n",
    "\n",
    "def noaa_input_event_type(driver):\n",
    "    driver.find_element_by_xpath(NOAA_EVENTTYPE_XPATH).click()\n",
    "\n",
    "def noaa_return_to_home(driver):\n",
    "    driver.find_element_by_xpath(NOAA_RETURN_XPATH).click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get text from the element in the main result page of NOAA\n",
    "def get_web_element_from_result_page(driver, xpath, index,base_xpath = \"//table[@id='results']//tr[position()>2 and position()<last()]\"):\n",
    "    return driver.find_element_by_xpath(f\"{base_xpath}[{index+1}]{xpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_inner_page(driver, xpath = '//td[1]//@href'):\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "def return_from_inner_page(driver, xpath = \"//*[@id='anch_9']\"):\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "    \n",
    "def get_inner_element_text(driver, xpath):\n",
    "    return driver.find_element_by_xpath(xpath).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_noaa(driver, curr_year, noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list,lat_lon_list,noaa_years_list):\n",
    "    noaa_base_xpath = \"//table[@id='results']//tr[position()>2 and position()<last()]\"\n",
    "    noaa_city_xpath = \"/td[1]//a\"\n",
    "    noaa_district_xpath = \"/td[2]\"\n",
    "    noaa_date_xpath = \"/td[4]\"\n",
    "    noaa_time_xpath = \"/td[5]\"\n",
    "    noaa_scale_xpath = \"/td[8]\"\n",
    "    noaa_deaths_xpath = \"/td[9]\"\n",
    "    return_from_inner_page_xpath = \"//*[@id='anch_9']\"\n",
    "    # inner_items_xpath = \"//table//tr[position() > 3 and position() < 7]/td[2]\"\n",
    "    length_inner_xpath = \"//table//tr/td[contains(text(), 'Length')]//following-sibling::td\"\n",
    "    width_inner_xpath = \"//table//tr/td[contains(text(), 'Width')]//following-sibling::td\"\n",
    "    country_inner_xpath = \"//table//tr/td[contains(text(), 'State')]//following-sibling::td\"\n",
    "    begin_lat_lon_xpath = \"//table//tr/td[contains(text(), 'Begin Lat/Lon')]//following-sibling::td\"\n",
    "    # noaa_click_to_inner_page_xpath = \"//td[1]//@href\"\n",
    "    ''' this will probably crash when the report number is 0, need to fix this'''\n",
    "    reports_count = len(driver.find_elements_by_xpath(f'{noaa_base_xpath}'))\n",
    "    for i in range(reports_count):\n",
    "        noaa_years_list.append(curr_year)\n",
    "        noaa_city_list.append(get_web_element_from_result_page(driver, noaa_city_xpath, i).text)\n",
    "        noaa_district_list.append(get_web_element_from_result_page(driver, noaa_district_xpath, i).text)\n",
    "        noaa_date_list.append(get_web_element_from_result_page(driver, noaa_date_xpath, i).text)\n",
    "        noaa_time_list.append(get_web_element_from_result_page(driver, noaa_time_xpath, i).text)\n",
    "        noaa_scale_list.append(get_web_element_from_result_page(driver, noaa_scale_xpath, i).text)\n",
    "        noaa_deaths.append(get_web_element_from_result_page(driver, noaa_deaths_xpath, i).text)\n",
    "        get_web_element_from_result_page(driver, noaa_city_xpath, i).click()\n",
    "        noaa_length_list.append(get_inner_element_text(driver, length_inner_xpath))\n",
    "        noaa_width_list.append(get_inner_element_text(driver, width_inner_xpath))\n",
    "        noaa_country_list.append(get_inner_element_text(driver, country_inner_xpath))\n",
    "        lat_lon_list.append(get_inner_element_text(driver, begin_lat_lon_xpath))\n",
    "        return_from_inner_page(driver, return_from_inner_page_xpath)\n",
    "    return noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list,lat_lon_list,noaa_years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list, lat_lon_list, noaa_years_list  = [], [], [], [], [], [], [] , [], [],[],[]\n",
    "driver_noaa = create_web_driver()\n",
    "start_web_driver(driver_noaa, NOAA_URL)\n",
    "# noaa_locations = driver_noaa.find_elements_by_xpath()\n",
    "# noaa_report_count = len(noaa_locations)\n",
    "for year in YEARS_TO_COUNT:\n",
    "    for month in range(1,3):\n",
    "        noaa_input_start_date(driver_noaa, year, month)\n",
    "        noaa_input_end_date(driver_noaa, year, month)\n",
    "        noaa_input_event_type(driver_noaa)\n",
    "        noaa_start_search(driver_noaa)\n",
    "        noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list, lat_lon_list, noaa_years_list = scrape_noaa(driver_noaa, year, noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list,lat_lon_list,noaa_years_list)\n",
    "        noaa_return_to_home(driver_noaa)\n",
    "usdf = pd.DataFrame({'City': noaa_city_list, 'District': noaa_district_list, 'Country': noaa_country_list, 'Date': noaa_date_list, 'Time': noaa_time_list, 'Scale': noaa_scale_list, 'Deaths': noaa_deaths, 'Length': noaa_length_list, 'Width': noaa_width_list, 'Lat Lon': lat_lon_list, 'Year': noaa_years_list})\n",
    "usdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_miles_to_km(miles):\n",
    "    return round(miles * 1.60934, 2)\n",
    "\n",
    "def convert_yards_to_meters(yards):\n",
    "    return round(yards * 0.9144, 2)\n",
    "\n",
    "def create_date_in_df(df):\n",
    "    df.BEGIN_YEARMONTH = df.BEGIN_YEARMONTH.astype(str)\n",
    "    df.BEGIN_DAY = df.BEGIN_DAY.astype(str)\n",
    "    df['Year'] = df.BEGIN_YEARMONTH.str[:4]\n",
    "    df['Month'] = df.BEGIN_YEARMONTH.str[4:]\n",
    "    df['Date'] = df.iloc[:,1].str.cat(df.iloc[:,13], sep='/').str.cat(df.iloc[:,12], sep='/')\n",
    "    df.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'Month', 'Year'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_csv_crawl():\n",
    "    start_csv_xpath = \"//a[contains(@href, 'details-ftp_v1.0_d\"\n",
    "    end_csv_xpath = \"')]\"\n",
    "    rename_dict = {\"STATE\": \"Country\",\"CZ_NAME\":\"District\", \"TOR_F_SCALE\": \"Scale\", \"TOR_LENGTH\":\"Length\",\"TOR_WIDTH\":\"Width\",\"BEGIN_LOCATION\":\"City\",\"BEGIN_LAT\":\"Latitude\",\"BEGIN_LON\":\"Longtitude\", \"DEATHS_DIRECT\": \"Deaths\", \"YEAR\": \"Year\"}\n",
    "    col_list = [\"BEGIN_YEARMONTH\", \"BEGIN_DAY\", \"STATE\", \"EVENT_TYPE\", \"CZ_NAME\", \"BEGIN_DATE_TIME\", \"DEATHS_DIRECT\", \"TOR_F_SCALE\", \"TOR_LENGTH\", \"TOR_WIDTH\", \"BEGIN_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\"]\n",
    "    df_list = []\n",
    "    csv_link = \"https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "    csv_driver = create_web_driver()\n",
    "    start_web_driver(csv_driver, csv_link)\n",
    "    time.sleep(1)\n",
    "    for i in range(1994,2018):\n",
    "        df_to_append = pd.read_csv(csv_link + csv_driver.find_element_by_xpath(f\"{start_csv_xpath}{i}{end_csv_xpath}\").text, compression='gzip', usecols=col_list)\n",
    "        df_to_append.rename(columns=rename_dict, inplace=True)\n",
    "        split_df = df_to_append.BEGIN_DATE_TIME.str.split(' ', expand=True)\n",
    "        df_to_append = df_to_append[df_to_append['EVENT_TYPE'] == 'Tornado'].drop(columns=['EVENT_TYPE', \"BEGIN_DATE_TIME\"])\n",
    "        df_to_append['Time'] = split_df[1].str[:5]\n",
    "        df_to_append = create_date_in_df(df_to_append)\n",
    "        df_to_append[\"Length\"] = df_to_append.Length.apply(lambda x: convert_miles_to_km(x))\n",
    "        df_to_append[\"Width\"] = df_to_append.Width.apply(lambda x: convert_yards_to_meters(x))\n",
    "        df_list.append(df_to_append)\n",
    "    csv_df = pd.concat(df_list, ignore_index=True)\n",
    "    csv_df = csv_df[[\"District\",\"City\",\"Country\",\"Longtitude\", \"Latitude\" ,\"Date\",\"Time\",\"Scale\",\"Length\", \"Width\", \"Deaths\"]]\n",
    "    csv_df.to_csv('noaa_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization for ESWD\n",
    "MONTHS_TO_COUNT = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "ESWD_URL = 'https://eswd.eu/cgi-bin/eswd.cgi'\n",
    "ESWD_TORNADO_XPATH = '//*[@name=\"TORNADO\"]'\n",
    "START_DATE_XPATH = '//*[@id=\"start_date\"]'\n",
    "END_DATE_XPATH = '//*[@id=\"end_date\"]'\n",
    "FIND_REPORTS_COUNT_XPATH = \"//p[contains(text(),'selected reports')] | //p[contains(text(),'no reports')]\"\n",
    "SUBMIT_XPATH = '(//*[@value=\"submit query\"])[2]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESWD auxilary functions for date parsing\n",
    "def get_end_date(curr_year, curr_month):\n",
    "    if curr_month == '02':\n",
    "        day = '28'\n",
    "    elif curr_month in ['04','06', '09','11']:\n",
    "        day = '30'\n",
    "    else:\n",
    "        day = '31'\n",
    "    return f'{day}-{curr_month}-{curr_year}'\n",
    "\n",
    "def get_start_date(curr_year,curr_month):\n",
    "    return f'01-{curr_month}-{curr_year}'\n",
    "\n",
    "def input_date(driver, start_date, end_date, start_date_xpath, end_date_xpath):\n",
    "    driver.find_element_by_xpath(start_date_xpath).clear()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(start_date_xpath).send_keys(start_date)\n",
    "    driver.find_element_by_xpath(end_date_xpath).clear()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(end_date_xpath).send_keys(end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to accumulate the data from the site in lists to prepare for dataframe creation using other auxilary functions\n",
    "def get_data_from_page(driver,district, city, country, long, lat, date, times, scale,length, width):\n",
    "    base_xpath =\"//td[@class='base_info']/p[b and not(@class='smallgray')]\"\n",
    "    district_xpath = \"/b[1]\"\n",
    "    bold_tags_xpath = \"/b\"\n",
    "    country_xpath = \"/b[2]\"\n",
    "    date_xpath = \"/b[3]\"\n",
    "    time_xpath = \"/b[4]\"\n",
    "    scale_xpath_1 = \"(//p[@class='TORNADO detail_info_entry'])\"\n",
    "    scale_xpath_2 = \"/b[contains(text(),'F')]\" \n",
    "    scale_xpath_3 = \"((//p[@class='TORNADO'][contains(b,'tornado')])\"\n",
    "    scale_xpath_4 = \"/b[text()='tornado'])\"\n",
    "    reports_count = len(driver.find_elements_by_xpath(base_xpath))\n",
    "    len_elements = driver.find_elements_by_xpath(scale_xpath_1)\n",
    "    parent_items= driver.find_elements_by_xpath(base_xpath)\n",
    "    for i in range(reports_count):\n",
    "        district = get_elements_from_list(driver,district_xpath,base_xpath,district,i)\n",
    "        child_item = driver.find_elements_by_xpath((f'({base_xpath})[{i+1}]{bold_tags_xpath}'))\n",
    "        child_item_text = get_text_from_element_list(child_item)\n",
    "        city= get_city_list(child_item_text,parent_items[i].text,city,0)\n",
    "        country = get_elements_from_list(driver,country_xpath,base_xpath,country,i)\n",
    "        long, lat =  get_long_lat_lists(child_item_text,parent_items[i].text,long,lat,1)\n",
    "        date = get_elements_from_list(driver,date_xpath,base_xpath,date,i)\n",
    "        times = get_elements_from_list(driver,time_xpath,base_xpath,times,i)\n",
    "        scale_elements_list = driver.find_elements_by_xpath((f'{scale_xpath_1}[{i+1}]{scale_xpath_2} | {scale_xpath_3}[{i+1}]{scale_xpath_4}'))\n",
    "        scale = get_scale_list(scale,scale_elements_list)\n",
    "        length = get_len_list(child_item_text,parent_items[i].text,length,1,len_elements[i])\n",
    "        width = get_width_list(width, len_elements[i])\n",
    "    return district, city, country, long,lat, date, times, scale,length,width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions in charge of initial parsing and cleaning of the data captured by XPATH\n",
    "def replace_child_text(child_text,field):\n",
    "    for word in child_text.split(' '):\n",
    "        field = field.replace('\\n','').replace(word,'',1).lstrip()\n",
    "    return field\n",
    "\n",
    "def checks_split_position(field,selector):\n",
    "    if field[0] == '(':\n",
    "        selector = selector + 1\n",
    "    return selector\n",
    "\n",
    "def checks_digit(field,selector):\n",
    "    return field.split('(')[selector][0].isdigit()\n",
    "\n",
    "def checks_len(field):\n",
    "    return '<' in field\n",
    "    \n",
    "def get_text_from_element_list(elem_list):\n",
    "    text = ''\n",
    "    for elem in elem_list:\n",
    "        text += elem.text + ' '\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to clean the data further and appending it to the relevant lists\n",
    "def convert_w_e_to_long(longtitude):\n",
    "    if 'W' in longtitude:\n",
    "        longtitude = longtitude.replace('W','').strip()\n",
    "        longtitude = -float(longtitude)\n",
    "    elif 'E' in longtitude:\n",
    "        longtitude = longtitude.replace('E','').strip()\n",
    "        longtitude = float(longtitude)\n",
    "    return longtitude\n",
    "def convert_n_to_lat(latitude):\n",
    "    latitude = latitude.replace('N','').strip()\n",
    "    latitude = float(latitude)\n",
    "    return latitude\n",
    "\n",
    "def append_coordinates_lists(field,long,lat,position):\n",
    "    regexp = re.compile(r'[0-9]*\\.[0-9]+ [a-zA-Z]+')\n",
    "    if checks_len(field):\n",
    "        coordinates = field.split('(')[position].split(')')[0].rstrip()\n",
    "    else:\n",
    "        coordinates = field.split('(')[position].rstrip().replace(')','')\n",
    "    indicator = field.split('(')[0]\n",
    "    if not re.match(regexp,coordinates):\n",
    "        long.append(np.nan)\n",
    "        lat.append(np.nan)\n",
    "    else:\n",
    "        longtitude = coordinates.split(',')[1]\n",
    "        longtitude = convert_w_e_to_long(longtitude)\n",
    "        latitude = coordinates.split(',')[0]\n",
    "        latitude = convert_n_to_lat(latitude)\n",
    "        long.append(longtitude)\n",
    "        lat.append(latitude)\n",
    "    return long,lat\n",
    "\n",
    "def get_long_lat_lists(child_text,field,long,lat,selector):\n",
    "    field = replace_child_text(child_text,field)\n",
    "    position = checks_split_position(field, selector)\n",
    "    if checks_digit(field,position):\n",
    "            long,lat = append_coordinates_lists(field,long,lat,position)\n",
    "    else:\n",
    "        if checks_digit(field,position-1):\n",
    "            long,lat = append_coordinates_lists(field,long,lat,position-1)\n",
    "        else:\n",
    "            long.append(np.nan)\n",
    "            lat.append(np.nan)\n",
    "    return long,lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to clean the data further and appending it to the relevant lists\n",
    "'''if the size of the elements list is 1 it gets only \"tornado\" dummy text, so there is no scale in the text, otherwise there is scale \n",
    "in the text so take the relevant F scale'''\n",
    "def get_scale_list(scale,scale_elements_list):\n",
    "    if len(scale_elements_list) == 1:\n",
    "        scale.append(np.nan)\n",
    "    else:\n",
    "        for element in scale_elements_list:\n",
    "            if 'F' in element.text:\n",
    "                scale.append(element.text)\n",
    "    return scale\n",
    "\n",
    "def get_width_list(curr_list, element):\n",
    "    if 'path width:' in element.text:\n",
    "        curr_list.append(element.text.split('path width:')[1].split('m')[0].strip()) \n",
    "    else:\n",
    "        curr_list.append(np.nan)\n",
    "    return curr_list\n",
    "\n",
    "'''if there's a field in the right textbox with specified path length get the number presented in it, otherwise check if the first char is a digit, if so check if the field on the left \n",
    "    has the \"<\" char, which indicates length, if so take it, otherwise input nan in the list'''\n",
    "def get_len_list(child_text,field,curr_list,selector,element):\n",
    "    if 'path length:' in element.text:\n",
    "        curr_list.append(element.text.split('path length:')[1].split('k')[0].strip()) \n",
    "    else:\n",
    "        field = replace_child_text(child_text,field)\n",
    "        if checks_digit(field,selector):\n",
    "            if checks_len(field):\n",
    "                curr_list.append(field.split('<')[1].split('k')[0].strip()) \n",
    "            else:\n",
    "                curr_list.append(np.nan)\n",
    "        else:\n",
    "            curr_list.append(np.nan)\n",
    "    return curr_list\n",
    "'''if the first character is a digit put nan, otherwise take the city name'''\n",
    "def get_city_list(child_text,field,curr_list,selector):\n",
    "    field = replace_child_text(child_text,field)\n",
    "    position = checks_split_position(field, selector)\n",
    "    if checks_digit(field,position):\n",
    "        curr_list.append(np.nan)\n",
    "    else:\n",
    "        curr_list.append(field.split('(')[position].rstrip().replace(')',''))\n",
    "    return curr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary functions to input bold text to the relevant lists and create a df using the lists\n",
    "def get_elements_from_list(driver,curr_xpath,base_xpath,curr_list,i):\n",
    "    curr_list.append(driver.find_elements_by_xpath((base_xpath+curr_xpath))[i].text)\n",
    "    return curr_list\n",
    "\n",
    "def get_df_from_lists(district, city, country, long, lat, date, times, scale,length,width):\n",
    "    df = pd.DataFrame({'district':district,'city':city,'country':country,'long':long,'lat':lat, 'date':date,'time':times,'scale':scale,'length (km)':length,'width (m)':width})\n",
    "    return df\n",
    "# def convert_directions_to_lat_lon(df):\n",
    "#     df['coordinates'] = df['coordinates'].astype(str)\n",
    "#     df['lat'] = df['coordinates'].str.split(',').str[0]\n",
    "#     df['lon'] = df['coordinates'].str.split(',').str[1]\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to crawl the data from the website and create a df\n",
    "def run_eswd_crawler():\n",
    "    driver = create_web_driver()\n",
    "    start_web_driver(driver,ESWD_URL)\n",
    "    driver.find_element_by_xpath(ESWD_TORNADO_XPATH).click()\n",
    "    district, city, country, long, lat, date, times, scale,length, width = [], [], [], [], [], [], [], [],[],[]\n",
    "    for year in YEARS_TO_COUNT:\n",
    "        for month in MONTHS_TO_COUNT:\n",
    "            end_date = get_end_date(year,month)\n",
    "            start_date = get_start_date(year, month)\n",
    "            input_date(driver, start_date, end_date,START_DATE_XPATH,END_DATE_XPATH)\n",
    "            driver.find_element_by_xpath(SUBMIT_XPATH).click()\n",
    "            reports_amount = driver.find_element_by_xpath(FIND_REPORTS_COUNT_XPATH).text\n",
    "            log(f'{year}-{month}-{reports_amount}', 'amount_of_reports.log')\n",
    "            if \"no report\" in reports_amount:\n",
    "                continue\n",
    "            district, city, country, long, lat, date, times, scale,length,width = get_data_from_page(driver,district, city, country, long, lat, date, times, scale,length,width)\n",
    "            edf = get_df_from_lists(district, city, country, long, lat, date, times, scale,length,width)\n",
    "    edf.to_csv(f'ESWD_data.csv', index=False)\n",
    "    close_web_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 96.0.4664\n",
      "Get LATEST chromedriver version for 96.0.4664 google-chrome\n",
      "Driver [/Users/elad/.wdm/drivers/chromedriver/mac64/96.0.4664.45/chromedriver] found in cache\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/1311800123.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  return webdriver.Chrome(ChromeDriverManager().install())\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/642766419.py:5: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(ESWD_TORNADO_XPATH).click()\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/519168234.py:15: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(start_date_xpath).clear()\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/519168234.py:17: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(start_date_xpath).send_keys(start_date)\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/519168234.py:18: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(end_date_xpath).clear()\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/519168234.py:20: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(end_date_xpath).send_keys(end_date)\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/642766419.py:12: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(SUBMIT_XPATH).click()\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/642766419.py:13: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  reports_amount = driver.find_element_by_xpath(FIND_REPORTS_COUNT_XPATH).text\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/2533196806.py:13: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  reports_count = len(driver.find_elements_by_xpath(base_xpath))\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/2533196806.py:14: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  len_elements = driver.find_elements_by_xpath(scale_xpath_1)\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/2533196806.py:15: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  parent_items= driver.find_elements_by_xpath(base_xpath)\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/303907199.py:3: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  curr_list.append(driver.find_elements_by_xpath((base_xpath+curr_xpath))[i].text)\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/2533196806.py:18: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  child_item = driver.find_elements_by_xpath((f'({base_xpath})[{i+1}]{bold_tags_xpath}'))\n",
      "/var/folders/gm/0p8xp0qn071cptf2y2k3ly7h0000gn/T/ipykernel_82931/2533196806.py:25: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  scale_elements_list = driver.find_elements_by_xpath((f'{scale_xpath_1}[{i+1}]{scale_xpath_2} | {scale_xpath_3}[{i+1}]{scale_xpath_4}'))\n"
     ]
    }
   ],
   "source": [
    "run_eswd_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'district', 'city', 'country', 'long', 'lat', 'date',\n",
       "       'time', 'scale', 'length (km)', 'width (m)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_df = pd.read_csv('ESWD_data.csv')\n",
    "my_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
