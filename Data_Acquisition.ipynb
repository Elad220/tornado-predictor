{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization\n",
    "YEARS_TO_COUNT = ['1990', '1991', '1992', '1993','1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl data from NOAA site, load CSV files to data frame and organize the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for data frame manipulation\n",
    "def convert_miles_to_km(miles):\n",
    "    return round(miles * 1.60934, 2)\n",
    "\n",
    "def convert_yards_to_meters(yards):\n",
    "    return round(yards * 0.9144, 2)\n",
    "\n",
    "def create_date_in_df(df):\n",
    "    df.BEGIN_YEARMONTH = df.BEGIN_YEARMONTH.astype(str)\n",
    "    df.BEGIN_DAY = df.BEGIN_DAY.astype(str)\n",
    "    df['Year'] = df.BEGIN_YEARMONTH.str[:4]\n",
    "    df['Month'] = df.BEGIN_YEARMONTH.str[4:]\n",
    "    df['Date'] = df.iloc[:,1].str.cat(df.iloc[:,13], sep='/').str.cat(df.iloc[:,12], sep='/')\n",
    "    df.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'Month', 'Year'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for csv scraping\n",
    "def getCSVSoup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_csv_url_from_soup(soup,year):\n",
    "    return soup.select(f'a[href*=\"details-ftp_v1.0_d{year}\"]')[0]['href']\n",
    "\n",
    "def get_df_bs_crawler():\n",
    "    noaa_csv_url = 'https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/'\n",
    "    col_list = [\"BEGIN_YEARMONTH\", \"BEGIN_DAY\", \"STATE\", \"EVENT_TYPE\", \"CZ_NAME\", \"BEGIN_DATE_TIME\", \"DEATHS_DIRECT\", \"TOR_F_SCALE\", \"TOR_LENGTH\", \"TOR_WIDTH\", \"BEGIN_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\"]\n",
    "    df_list = []\n",
    "    for year in YEARS_TO_COUNT:\n",
    "        soup = getCSVSoup(noaa_csv_url)\n",
    "        csv_url = get_csv_url_from_soup(soup, year)\n",
    "        df_list.append(pd.read_csv(f'{noaa_csv_url}{csv_url}', compression='gzip', usecols=col_list))\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def clean_df_csv(df):\n",
    "    rename_dict = {\"STATE\": \"Country\",\"CZ_NAME\":\"District\", \"TOR_F_SCALE\": \"Scale\", \"TOR_LENGTH\":\"Length (KM)\",\"TOR_WIDTH\":\"Width (M)\",\"BEGIN_LOCATION\":\"City\",\"BEGIN_LAT\":\"Latitude\",\"BEGIN_LON\":\"Longtitude\", \"DEATHS_DIRECT\": \"Deaths\", \"YEAR\": \"Year\"}\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    split_df = df.BEGIN_DATE_TIME.str.split(' ', expand=True)\n",
    "    df = df[df['EVENT_TYPE'] == 'Tornado'].drop(columns=['EVENT_TYPE', \"BEGIN_DATE_TIME\"])\n",
    "    df['Time'] = split_df[1].str[:5]\n",
    "    df = create_date_in_df(df)\n",
    "    df['Length (KM)'] = df['Length (KM)'].apply(lambda x: convert_miles_to_km(x))\n",
    "    df[\"Width (M)\"] = df['Width (M)'].apply(lambda x: convert_yards_to_meters(x))\n",
    "    df = df[[\"District\",\"City\",\"Country\",\"Longtitude\", \"Latitude\" ,\"Date\",\"Time\",\"Scale\",\"Length (KM)\", \"Width (M)\", \"Deaths\"]]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csvs():\n",
    "    noaadf = pd.read_csv('noaa_data.csv')\n",
    "    eswddf = pd.read_csv('eswd_data.csv')\n",
    "    list = [noaadf, eswddf]\n",
    "    df = pd.concat(list, ignore_index=True)\n",
    "    df.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate csv crawler\n",
    "def run_csv_crawl():\n",
    "    start_csv_xpath = \"//a[contains(@href, 'details-ftp_v1.0_d\"\n",
    "    end_csv_xpath = \"')]\"\n",
    "    rename_dict = {\"STATE\": \"Country\",\"CZ_NAME\":\"District\", \"TOR_F_SCALE\": \"Scale\", \"TOR_LENGTH\":\"Length\",\"TOR_WIDTH\":\"Width\",\"BEGIN_LOCATION\":\"City\",\"BEGIN_LAT\":\"Latitude\",\"BEGIN_LON\":\"Longtitude\", \"DEATHS_DIRECT\": \"Deaths\", \"YEAR\": \"Year\"}\n",
    "    col_list = [\"BEGIN_YEARMONTH\", \"BEGIN_DAY\", \"STATE\", \"EVENT_TYPE\", \"CZ_NAME\", \"BEGIN_DATE_TIME\", \"DEATHS_DIRECT\", \"TOR_F_SCALE\", \"TOR_LENGTH\", \"TOR_WIDTH\", \"BEGIN_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\"]\n",
    "    df_list = []\n",
    "    csv_link = \"https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "    csv_driver = create_web_driver()\n",
    "    start_web_driver(csv_driver, csv_link)\n",
    "    time.sleep(1)\n",
    "    for i in range(1994,2018):\n",
    "        df_to_append = pd.read_csv(csv_link + csv_driver.find_element_by_xpath(f\"{start_csv_xpath}{i}{end_csv_xpath}\").text, compression='gzip', usecols=col_list)\n",
    "        df_to_append.rename(columns=rename_dict, inplace=True)\n",
    "        split_df = df_to_append.BEGIN_DATE_TIME.str.split(' ', expand=True)\n",
    "        df_to_append = df_to_append[df_to_append['EVENT_TYPE'] == 'Tornado'].drop(columns=['EVENT_TYPE', \"BEGIN_DATE_TIME\"])\n",
    "        df_to_append['Time'] = split_df[1].str[:5]\n",
    "        df_to_append = create_date_in_df(df_to_append)\n",
    "        df_to_append[\"Length\"] = df_to_append.Length.apply(lambda x: convert_miles_to_km(x))\n",
    "        df_to_append[\"Width\"] = df_to_append.Width.apply(lambda x: convert_yards_to_meters(x))\n",
    "        df_list.append(df_to_append)\n",
    "    csv_df = pd.concat(df_list, ignore_index=True)\n",
    "    csv_df = csv_df[[\"District\",\"City\",\"Country\",\"Longtitude\", \"Latitude\" ,\"Date\",\"Time\",\"Scale\",\"Length\", \"Width\", \"Deaths\"]]\n",
    "    csv_df.to_csv('noaa_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_noaa_crawl():\n",
    "    noaadf = get_df_bs_crawler()\n",
    "    noaadf = clean_df_csv(noaadf)\n",
    "    noaadf.to_csv('noaa_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run web crawler on ESWD site, scrape each query for relevant data, and put it in a dataframe for further analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions for creating the web driver and closing it\n",
    "def create_web_driver():\n",
    "    return webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "def start_web_driver(driver, url):\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "\n",
    "def close_web_driver(driver):\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization for ESWD\n",
    "MONTHS_TO_COUNT = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "ESWD_URL = 'https://eswd.eu/cgi-bin/eswd.cgi'\n",
    "ESWD_TORNADO_XPATH = '//*[@name=\"TORNADO\"]'\n",
    "START_DATE_XPATH = '//*[@id=\"start_date\"]'\n",
    "END_DATE_XPATH = '//*[@id=\"end_date\"]'\n",
    "FIND_REPORTS_COUNT_XPATH = \"//p[contains(text(),'selected reports')] | //p[contains(text(),'no reports')]\"\n",
    "SUBMIT_XPATH = '(//*[@value=\"submit query\"])[2]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESWD auxilary functions for date parsing\n",
    "def get_end_date(curr_year, curr_month):\n",
    "    if curr_month == '02':\n",
    "        day = '28'\n",
    "    elif curr_month in ['04','06', '09','11']:\n",
    "        day = '30'\n",
    "    else:\n",
    "        day = '31'\n",
    "    return f'{day}-{curr_month}-{curr_year}'\n",
    "\n",
    "def get_start_date(curr_year,curr_month):\n",
    "    return f'01-{curr_month}-{curr_year}'\n",
    "\n",
    "def input_date(driver, start_date, end_date, start_date_xpath, end_date_xpath):\n",
    "    driver.find_element_by_xpath(start_date_xpath).clear()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(start_date_xpath).send_keys(start_date)\n",
    "    driver.find_element_by_xpath(end_date_xpath).clear()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(end_date_xpath).send_keys(end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to accumulate the data from the site in lists to prepare for dataframe creation using other auxilary functions\n",
    "def get_data_from_page(driver,district, city, country, long, lat, date, times, scale,length, width,deaths):\n",
    "    base_xpath =\"//td[@class='base_info']/p[b and not(@class='smallgray')]\"\n",
    "    district_xpath = \"/b[1]\"\n",
    "    bold_tags_xpath = \"/b\"\n",
    "    country_xpath = \"/b[2]\"\n",
    "    date_xpath = \"/b[3]\"\n",
    "    time_xpath = \"/b[4]\"\n",
    "    scale_xpath_1 = \"(//p[@class='TORNADO detail_info_entry'])\"\n",
    "    scale_xpath_2 = \"/b[contains(text(),'F')]\" \n",
    "    scale_xpath_3 = \"((//p[@class='TORNADO'][contains(b,'tornado')])\"\n",
    "    scale_xpath_4 = \"/b[text()='tornado'])\"\n",
    "    reports_count = len(driver.find_elements_by_xpath(base_xpath))\n",
    "    len_elements = driver.find_elements_by_xpath(scale_xpath_1)\n",
    "    parent_items= driver.find_elements_by_xpath(base_xpath)\n",
    "    for i in range(reports_count):\n",
    "        district = get_elements_from_list(driver,district_xpath,base_xpath,district,i)\n",
    "        child_item = driver.find_elements_by_xpath((f'({base_xpath})[{i+1}]{bold_tags_xpath}'))\n",
    "        child_item_text = get_text_from_element_list(child_item)\n",
    "        city= get_city_list(child_item_text,parent_items[i].text,city,0)\n",
    "        country = get_elements_from_list(driver,country_xpath,base_xpath,country,i)\n",
    "        long, lat =  get_long_lat_lists(child_item_text,parent_items[i].text,long,lat,1)\n",
    "        date = get_elements_from_list(driver,date_xpath,base_xpath,date,i)\n",
    "        times = get_elements_from_list(driver,time_xpath,base_xpath,times,i)\n",
    "        scale_elements_list = driver.find_elements_by_xpath((f'{scale_xpath_1}[{i+1}]{scale_xpath_2} | {scale_xpath_3}[{i+1}]{scale_xpath_4}'))\n",
    "        scale = get_scale_list(scale,scale_elements_list)\n",
    "        length = get_len_list(child_item_text,parent_items[i].text,length,1,len_elements[i])\n",
    "        width = get_width_list(width, len_elements[i])\n",
    "        deaths = get_deaths_list(deaths,len_elements[i])\n",
    "    return district, city, country, long,lat, date, times, scale,length,width,deaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions in charge of initial parsing and cleaning of the data captured by XPATH\n",
    "def replace_child_text(child_text,field):\n",
    "    for word in child_text.split(' '):\n",
    "        field = field.replace('\\n','').replace(word,'',1).lstrip()\n",
    "    return field\n",
    "\n",
    "def checks_split_position(field,selector):\n",
    "    if field[0] == '(':\n",
    "        selector = selector + 1\n",
    "    return selector\n",
    "\n",
    "def checks_digit(field,selector):\n",
    "    return field.split('(')[selector][0].isdigit()\n",
    "\n",
    "def checks_len(field):\n",
    "    return '<' in field\n",
    "    \n",
    "def get_text_from_element_list(elem_list):\n",
    "    text = ''\n",
    "    for elem in elem_list:\n",
    "        text += elem.text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to clean the data further and appending it to the relevant lists\n",
    "def convert_w_e_to_long(longtitude):\n",
    "    if 'W' in longtitude:\n",
    "        longtitude = longtitude.replace('W','').strip()\n",
    "        longtitude = -float(longtitude)\n",
    "    elif 'E' in longtitude:\n",
    "        longtitude = longtitude.replace('E','').strip()\n",
    "        longtitude = float(longtitude)\n",
    "    return longtitude\n",
    "\n",
    "def convert_n_to_lat(latitude):\n",
    "    latitude = latitude.replace('N','').strip()\n",
    "    latitude = float(latitude)\n",
    "    return latitude\n",
    "\n",
    "def append_coordinates_lists(field,long,lat,position):\n",
    "    regexp = re.compile(r'[0-9]*\\.[0-9]+ [a-zA-Z]+')\n",
    "    if checks_len(field):\n",
    "        coordinates = field.split('(')[position].split(')')[0].rstrip()\n",
    "    else:\n",
    "        coordinates = field.split('(')[position].rstrip().replace(')','')\n",
    "    if not re.match(regexp,coordinates):\n",
    "        long.append(np.nan)\n",
    "        lat.append(np.nan)\n",
    "    else:\n",
    "        longtitude = coordinates.split(',')[1]\n",
    "        longtitude = convert_w_e_to_long(longtitude)\n",
    "        latitude = coordinates.split(',')[0]\n",
    "        latitude = convert_n_to_lat(latitude)\n",
    "        long.append(longtitude)\n",
    "        lat.append(latitude)\n",
    "    return long,lat\n",
    "\n",
    "def get_long_lat_lists(child_text,field,long,lat,selector):\n",
    "    field = replace_child_text(child_text,field)\n",
    "    position = checks_split_position(field, selector)\n",
    "    if checks_digit(field,position):\n",
    "            long,lat = append_coordinates_lists(field,long,lat,position)\n",
    "    else:\n",
    "        if checks_digit(field,position-1):\n",
    "            long,lat = append_coordinates_lists(field,long,lat,position-1)\n",
    "        else:\n",
    "            long.append(np.nan)\n",
    "            lat.append(np.nan)\n",
    "    return long,lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deaths_list(curr_list, element):\n",
    "    if 'dead:' in element.text:\n",
    "        curr_list.append(element.text.split('dead:')[1].split('.')[0])\n",
    "    else:\n",
    "        curr_list.append(np.nan)\n",
    "    return curr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to clean the data further and appending it to the relevant lists\n",
    "'''if the size of the elements list is 1 it gets only \"tornado\" dummy text, so there is no scale in the text, otherwise there is scale \n",
    "in the text so take the relevant F scale'''\n",
    "def get_scale_list(scale,scale_elements_list):\n",
    "    if len(scale_elements_list) == 1:\n",
    "        scale.append(np.nan)\n",
    "    else:\n",
    "        for element in scale_elements_list:\n",
    "            if 'F' in element.text:\n",
    "                scale.append(element.text)\n",
    "    return scale\n",
    "\n",
    "def get_width_list(curr_list, element):\n",
    "    if 'path width:' in element.text:\n",
    "        curr_list.append(element.text.split('path width:')[1].split('m')[0].strip()) \n",
    "    else:\n",
    "        curr_list.append(np.nan)\n",
    "    return curr_list\n",
    "\n",
    "'''if there's a field in the right textbox with specified path length get the number presented in it, otherwise check if the first char is a digit,\n",
    " if so check if the field on the left has the \"<\" char, which indicates length, if so take it, otherwise input nan in the list'''\n",
    "def get_len_list(child_text,field,curr_list,selector,element):\n",
    "    if 'path length:' in element.text:\n",
    "        curr_list.append(element.text.split('path length:')[1].split('k')[0].strip()) \n",
    "    else:\n",
    "        field = replace_child_text(child_text,field)\n",
    "        if checks_digit(field,selector):\n",
    "            if checks_len(field):\n",
    "                curr_list.append(field.split('<')[1].split('k')[0].strip()) \n",
    "            else:\n",
    "                curr_list.append(np.nan)\n",
    "        else:\n",
    "            curr_list.append(np.nan)\n",
    "    return curr_list\n",
    "\n",
    "'''if the first character is a digit put nan, otherwise take the city name'''\n",
    "def get_city_list(child_text,field,curr_list,selector):\n",
    "    field = replace_child_text(child_text,field)\n",
    "    position = checks_split_position(field, selector)\n",
    "    if checks_digit(field,position):\n",
    "        curr_list.append(np.nan)\n",
    "    else:\n",
    "        curr_list.append(field.split('(')[position].rstrip().replace(')',''))\n",
    "    return curr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary functions to input bold text to the relevant lists and create a df using the lists\n",
    "def get_elements_from_list(driver,curr_xpath,base_xpath,curr_list,i):\n",
    "    curr_list.append(driver.find_elements_by_xpath((base_xpath+curr_xpath))[i].text)\n",
    "    return curr_list\n",
    "\n",
    "def get_df_from_lists(district, city, country, long, lat, date, times, scale,length,width,deaths):\n",
    "    df = pd.DataFrame({'District':district,'City':city,'Country':country,'Longtitude':long,'Latitude':lat, 'Date':date,'Time':times,'Scale':scale,'Length (KM)':length,'Width (M)':width,'Deaths':deaths})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to crawl the data from the website and create a df\n",
    "def run_eswd_crawler():\n",
    "    driver = create_web_driver()\n",
    "    start_web_driver(driver,ESWD_URL)\n",
    "    driver.find_element_by_xpath(ESWD_TORNADO_XPATH).click()\n",
    "    district, city, country, long, lat, date, times, scale,length, width,deaths = [], [], [], [], [], [], [], [],[],[],[]\n",
    "    for year in YEARS_TO_COUNT:\n",
    "        for month in MONTHS_TO_COUNT:\n",
    "            end_date = get_end_date(year,month)\n",
    "            start_date = get_start_date(year, month)\n",
    "            input_date(driver, start_date, end_date,START_DATE_XPATH,END_DATE_XPATH)\n",
    "            driver.find_element_by_xpath(SUBMIT_XPATH).click()\n",
    "            reports_amount = driver.find_element_by_xpath(FIND_REPORTS_COUNT_XPATH).text\n",
    "            if \"no report\" in reports_amount:\n",
    "                continue\n",
    "            district, city, country, long, lat, date, times, scale,length,width,deaths = get_data_from_page(driver,district, city, country, long, lat, date, times, scale,length,width,deaths)\n",
    "            edf = get_df_from_lists(district, city, country, long, lat, date, times, scale,length,width,deaths)\n",
    "    edf.to_csv(f'ESWD_data.csv', index=False)\n",
    "    close_web_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_noaa_crawl()\n",
    "run_eswd_crawler()\n",
    "merge_csvs()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
