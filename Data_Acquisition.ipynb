{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization\n",
    "YEARS_TO_COUNT = ['1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions for creating the web driver and closing it\n",
    "def create_web_driver():\n",
    "    return webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "def start_web_driver(driver, url):\n",
    "    driver.maximize_window()\n",
    "    driver.get(url)\n",
    "\n",
    "def close_web_driver(driver):\n",
    "    driver.close()\n",
    "\n",
    "def log(message,filename):\n",
    "    with open(f'{filename}', 'a') as f:\n",
    "        f.write(message + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization for NOAA\n",
    "NOAA_URL = 'https://www.ncdc.noaa.gov/stormevents/choosedates.jsp?statefips=-999,ALL'\n",
    "NOAA_BEGINMONTH_XPATH = \"//select[@id='beginDate_mm']//option[contains(text(),'\"\n",
    "NOAA_BEGINYEAR_XPATH = \"//select[@id='beginDate_yyyy']//option[contains(text(),'\"\n",
    "NOAA_BEGINDAY_XPATH = \"//select[@id='beginDate_dd']//option[contains(text(),'01')]\"\n",
    "NOAA_ENDMONTH_XPATH = \"//select[@id='endDate_mm']//option[contains(text(),'\"\n",
    "NOAA_ENDYEAR_XPATH = \"//select[@id='endDate_yyyy']//option[contains(text(),'\"\n",
    "NOAA_ENDDAY_XPATH = \"//select[@id='endDate_dd']//option[contains(text(),'\"\n",
    "NOAA_EVENTTYPE_XPATH = \"//select[@id='eventType']//option[contains(@value,'Tornado')]\"\n",
    "NOAA_SEARCH_XPATH = \"//input[@value='Search']\"\n",
    "NOAA_RETURN_XPATH = '//*[@id=\"anch_8\"]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for NOAA to parse the relevant date and input it in the relevant fields of the web driver received by input_date\n",
    "def noaa_input_start_date(driver, year, month):\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINMONTH_XPATH}{month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINMONTH_XPATH}{month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINYEAR_XPATH}{year}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINYEAR_XPATH}{year}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINDAY_XPATH}\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_BEGINDAY_XPATH}\").click()\n",
    "\n",
    "def noaa_input_end_date(driver, curr_year, end_month):\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDMONTH_XPATH}{end_month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDMONTH_XPATH}{end_month}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDYEAR_XPATH}{curr_year}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDYEAR_XPATH}{curr_year}')]\").click()\n",
    "    if end_month == 2:\n",
    "        end_day = 28\n",
    "    elif end_month in [4,6,7,9,11]:\n",
    "        end_day = 30\n",
    "    else:\n",
    "        end_day = 31\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDDAY_XPATH}{end_day}')]\").click()\n",
    "    driver.find_element_by_xpath(f\"{NOAA_ENDDAY_XPATH}{end_day}')]\").click()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to click the relevant fields in NOAA web driver\n",
    "def noaa_start_search(driver):\n",
    "    driver.find_element_by_xpath(NOAA_SEARCH_XPATH).click()\n",
    "\n",
    "def noaa_input_event_type(driver):\n",
    "    driver.find_element_by_xpath(NOAA_EVENTTYPE_XPATH).click()\n",
    "\n",
    "def noaa_return_to_home(driver):\n",
    "    driver.find_element_by_xpath(NOAA_RETURN_XPATH).click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get text from the element in the main result page of NOAA\n",
    "def get_web_element_from_result_page(driver, xpath, index,base_xpath = \"//table[@id='results']//tr[position()>2 and position()<last()]\"):\n",
    "    return driver.find_element_by_xpath(f\"{base_xpath}[{index+1}]{xpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_inner_page(driver, xpath = '//td[1]//@href'):\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "def return_from_inner_page(driver, xpath = \"//*[@id='anch_9']\"):\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "    \n",
    "def get_inner_element_text(driver, xpath):\n",
    "    return driver.find_element_by_xpath(xpath).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_noaa(driver, curr_year, noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list,lat_lon_list,noaa_years_list):\n",
    "    noaa_base_xpath = \"//table[@id='results']//tr[position()>2 and position()<last()]\"\n",
    "    noaa_city_xpath = \"/td[1]//a\"\n",
    "    noaa_district_xpath = \"/td[2]\"\n",
    "    noaa_date_xpath = \"/td[4]\"\n",
    "    noaa_time_xpath = \"/td[5]\"\n",
    "    noaa_scale_xpath = \"/td[8]\"\n",
    "    noaa_deaths_xpath = \"/td[9]\"\n",
    "    return_from_inner_page_xpath = \"//*[@id='anch_9']\"\n",
    "    # inner_items_xpath = \"//table//tr[position() > 3 and position() < 7]/td[2]\"\n",
    "    length_inner_xpath = \"//table//tr/td[contains(text(), 'Length')]//following-sibling::td\"\n",
    "    width_inner_xpath = \"//table//tr/td[contains(text(), 'Width')]//following-sibling::td\"\n",
    "    country_inner_xpath = \"//table//tr/td[contains(text(), 'State')]//following-sibling::td\"\n",
    "    begin_lat_lon_xpath = \"//table//tr/td[contains(text(), 'Begin Lat/Lon')]//following-sibling::td\"\n",
    "    # noaa_click_to_inner_page_xpath = \"//td[1]//@href\"\n",
    "    ''' this will probably crash when the report number is 0, need to fix this'''\n",
    "    reports_count = len(driver.find_elements_by_xpath(f'{noaa_base_xpath}'))\n",
    "    for i in range(reports_count):\n",
    "        noaa_years_list.append(curr_year)\n",
    "        noaa_city_list.append(get_web_element_from_result_page(driver, noaa_city_xpath, i).text)\n",
    "        noaa_district_list.append(get_web_element_from_result_page(driver, noaa_district_xpath, i).text)\n",
    "        noaa_date_list.append(get_web_element_from_result_page(driver, noaa_date_xpath, i).text)\n",
    "        noaa_time_list.append(get_web_element_from_result_page(driver, noaa_time_xpath, i).text)\n",
    "        noaa_scale_list.append(get_web_element_from_result_page(driver, noaa_scale_xpath, i).text)\n",
    "        noaa_deaths.append(get_web_element_from_result_page(driver, noaa_deaths_xpath, i).text)\n",
    "        get_web_element_from_result_page(driver, noaa_city_xpath, i).click()\n",
    "        noaa_length_list.append(get_inner_element_text(driver, length_inner_xpath))\n",
    "        noaa_width_list.append(get_inner_element_text(driver, width_inner_xpath))\n",
    "        noaa_country_list.append(get_inner_element_text(driver, country_inner_xpath))\n",
    "        lat_lon_list.append(get_inner_element_text(driver, begin_lat_lon_xpath))\n",
    "        return_from_inner_page(driver, return_from_inner_page_xpath)\n",
    "    return noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list,lat_lon_list,noaa_years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list, lat_lon_list, noaa_years_list  = [], [], [], [], [], [], [] , [], [],[],[]\n",
    "driver_noaa = create_web_driver()\n",
    "start_web_driver(driver_noaa, NOAA_URL)\n",
    "# noaa_locations = driver_noaa.find_elements_by_xpath()\n",
    "# noaa_report_count = len(noaa_locations)\n",
    "for year in YEARS_TO_COUNT:\n",
    "    for month in range(1,3):\n",
    "        noaa_input_start_date(driver_noaa, year, month)\n",
    "        noaa_input_end_date(driver_noaa, year, month)\n",
    "        noaa_input_event_type(driver_noaa)\n",
    "        noaa_start_search(driver_noaa)\n",
    "        noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list, lat_lon_list, noaa_years_list = scrape_noaa(driver_noaa, year, noaa_city_list, noaa_district_list, noaa_country_list, noaa_date_list, noaa_time_list, noaa_scale_list, noaa_deaths, noaa_length_list, noaa_width_list,lat_lon_list,noaa_years_list)\n",
    "        noaa_return_to_home(driver_noaa)\n",
    "usdf = pd.DataFrame({'City': noaa_city_list, 'District': noaa_district_list, 'Country': noaa_country_list, 'Date': noaa_date_list, 'Time': noaa_time_list, 'Scale': noaa_scale_list, 'Deaths': noaa_deaths, 'Length': noaa_length_list, 'Width': noaa_width_list, 'Lat Lon': lat_lon_list, 'Year': noaa_years_list})\n",
    "usdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_miles_to_km(miles):\n",
    "    return round(miles * 1.60934, 2)\n",
    "\n",
    "def convert_yards_to_meters(yards):\n",
    "    return round(yards * 0.9144, 2)\n",
    "\n",
    "def create_date_in_df(df):\n",
    "    df.BEGIN_YEARMONTH = df.BEGIN_YEARMONTH.astype(str)\n",
    "    df.BEGIN_DAY = df.BEGIN_DAY.astype(str)\n",
    "    df['Year'] = df.BEGIN_YEARMONTH.str[:4]\n",
    "    df['Month'] = df.BEGIN_YEARMONTH.str[4:]\n",
    "    df['Date'] = df.iloc[:,1].str.cat(df.iloc[:,13], sep='/').str.cat(df.iloc[:,12], sep='/')\n",
    "    df.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'Month', 'Year'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCSVSoup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_csv_url_from_soup(soup,year):\n",
    "    # return soup.find('a', href=f'StormEvents_details-ftp_v1.0_d{year}_c20210803.csv.gz')\n",
    "    return soup.select(f'a[href*=\"details-ftp_v1.0_d{year}\"]')[0]['href']\n",
    "\n",
    "def get_df_bs_crawler():\n",
    "    noaa_csv_url = 'https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/'\n",
    "    col_list = [\"BEGIN_YEARMONTH\", \"BEGIN_DAY\", \"STATE\", \"EVENT_TYPE\", \"CZ_NAME\", \"BEGIN_DATE_TIME\", \"DEATHS_DIRECT\", \"TOR_F_SCALE\", \"TOR_LENGTH\", \"TOR_WIDTH\", \"BEGIN_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\"]\n",
    "    df_list = []\n",
    "    for year in YEARS_TO_COUNT:\n",
    "        soup = getCSVSoup(noaa_csv_url)\n",
    "        csv_url = get_csv_url_from_soup(soup, year)\n",
    "        df_list.append(pd.read_csv(f'{noaa_csv_url}{csv_url}', compression='gzip', usecols=col_list))\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def clean_df_csv(df):\n",
    "    rename_dict = {\"STATE\": \"Country\",\"CZ_NAME\":\"District\", \"TOR_F_SCALE\": \"Scale\", \"TOR_LENGTH\":\"Length (KM)\",\"TOR_WIDTH\":\"Width (M)\",\"BEGIN_LOCATION\":\"City\",\"BEGIN_LAT\":\"Latitude\",\"BEGIN_LON\":\"Longtitude\", \"DEATHS_DIRECT\": \"Deaths\", \"YEAR\": \"Year\"}\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    split_df = df.BEGIN_DATE_TIME.str.split(' ', expand=True)\n",
    "    df = df[df['EVENT_TYPE'] == 'Tornado'].drop(columns=['EVENT_TYPE', \"BEGIN_DATE_TIME\"])\n",
    "    df['Time'] = split_df[1].str[:5]\n",
    "    df = create_date_in_df(df)\n",
    "    df['Length (KM)'] = df['Length (KM)'].apply(lambda x: convert_miles_to_km(x))\n",
    "    df[\"Width (M)\"] = df['Width (M)'].apply(lambda x: convert_yards_to_meters(x))\n",
    "    df = df[[\"District\",\"City\",\"Country\",\"Longtitude\", \"Latitude\" ,\"Date\",\"Time\",\"Scale\",\"Length (KM)\", \"Width (M)\", \"Deaths\"]]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Length (KM)</th>\n",
       "      <th>Width (M)</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEKALB</td>\n",
       "      <td>Grove Oak to Rainsvil</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>-85.9800</td>\n",
       "      <td>34.4300</td>\n",
       "      <td>27/03/1994</td>\n",
       "      <td>11:32</td>\n",
       "      <td>F4</td>\n",
       "      <td>37.01</td>\n",
       "      <td>640.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUSCALOOSA</td>\n",
       "      <td>Tuscaloosa</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27/03/1994</td>\n",
       "      <td>16:02</td>\n",
       "      <td>F1</td>\n",
       "      <td>3.22</td>\n",
       "      <td>91.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARSHALL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23/02/1994</td>\n",
       "      <td>03:40</td>\n",
       "      <td>F0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>18.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEE</td>\n",
       "      <td>Opelika</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>-85.3700</td>\n",
       "      <td>32.4800</td>\n",
       "      <td>2/08/1994</td>\n",
       "      <td>17:25</td>\n",
       "      <td>F0</td>\n",
       "      <td>4.18</td>\n",
       "      <td>274.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BLOUNT</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>-86.5700</td>\n",
       "      <td>33.9800</td>\n",
       "      <td>28/11/1994</td>\n",
       "      <td>00:30</td>\n",
       "      <td>F1</td>\n",
       "      <td>2.57</td>\n",
       "      <td>91.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32516</th>\n",
       "      <td>NODAWAY</td>\n",
       "      <td>SKIDMORE</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>-95.0900</td>\n",
       "      <td>40.3100</td>\n",
       "      <td>6/03/2017</td>\n",
       "      <td>18:30</td>\n",
       "      <td>EF0</td>\n",
       "      <td>9.33</td>\n",
       "      <td>22.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32517</th>\n",
       "      <td>LAFAYETTE</td>\n",
       "      <td>MAYVIEW</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>-93.8511</td>\n",
       "      <td>39.0129</td>\n",
       "      <td>6/03/2017</td>\n",
       "      <td>20:34</td>\n",
       "      <td>EF1</td>\n",
       "      <td>2.82</td>\n",
       "      <td>45.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32518</th>\n",
       "      <td>MONMOUTH</td>\n",
       "      <td>LOWER SQUANKUM</td>\n",
       "      <td>NEW JERSEY</td>\n",
       "      <td>-74.1830</td>\n",
       "      <td>40.1720</td>\n",
       "      <td>24/06/2017</td>\n",
       "      <td>06:27</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>22.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32519</th>\n",
       "      <td>HARDIN</td>\n",
       "      <td>OLIVEHILL</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>-88.0316</td>\n",
       "      <td>35.2669</td>\n",
       "      <td>31/08/2017</td>\n",
       "      <td>15:37</td>\n",
       "      <td>EF0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>64.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32520</th>\n",
       "      <td>ITAWAMBA</td>\n",
       "      <td>KIRKVILLE</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>-88.5135</td>\n",
       "      <td>34.4294</td>\n",
       "      <td>31/08/2017</td>\n",
       "      <td>12:23</td>\n",
       "      <td>EF1</td>\n",
       "      <td>3.98</td>\n",
       "      <td>137.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32521 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         District                   City      Country  Longtitude  Latitude  \\\n",
       "0          DEKALB  Grove Oak to Rainsvil      ALABAMA    -85.9800   34.4300   \n",
       "1      TUSCALOOSA             Tuscaloosa      ALABAMA         NaN       NaN   \n",
       "2        MARSHALL                    NaN      ALABAMA         NaN       NaN   \n",
       "3             LEE                Opelika      ALABAMA    -85.3700   32.4800   \n",
       "4          BLOUNT              Cleveland      ALABAMA    -86.5700   33.9800   \n",
       "...           ...                    ...          ...         ...       ...   \n",
       "32516     NODAWAY               SKIDMORE     MISSOURI    -95.0900   40.3100   \n",
       "32517   LAFAYETTE                MAYVIEW     MISSOURI    -93.8511   39.0129   \n",
       "32518    MONMOUTH         LOWER SQUANKUM   NEW JERSEY    -74.1830   40.1720   \n",
       "32519      HARDIN              OLIVEHILL    TENNESSEE    -88.0316   35.2669   \n",
       "32520    ITAWAMBA              KIRKVILLE  MISSISSIPPI    -88.5135   34.4294   \n",
       "\n",
       "             Date   Time Scale  Length (KM)  Width (M)  Deaths  \n",
       "0      27/03/1994  11:32    F4        37.01     640.08       0  \n",
       "1      27/03/1994  16:02    F1         3.22      91.44       0  \n",
       "2      23/02/1994  03:40    F0         0.48      18.29       0  \n",
       "3       2/08/1994  17:25    F0         4.18     274.32       0  \n",
       "4      28/11/1994  00:30    F1         2.57      91.44       0  \n",
       "...           ...    ...   ...          ...        ...     ...  \n",
       "32516   6/03/2017  18:30   EF0         9.33      22.86       0  \n",
       "32517   6/03/2017  20:34   EF1         2.82      45.72       0  \n",
       "32518  24/06/2017  06:27   EF0         0.48      22.86       0  \n",
       "32519  31/08/2017  15:37   EF0         1.21      64.01       0  \n",
       "32520  31/08/2017  12:23   EF1         3.98     137.16       0  \n",
       "\n",
       "[32521 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noaadf = get_df_bs_crawler()\n",
    "noaadf = clean_df_csv(noaadf)\n",
    "noaadf.to_csv('noaa_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_csv_crawl():\n",
    "    start_csv_xpath = \"//a[contains(@href, 'details-ftp_v1.0_d\"\n",
    "    end_csv_xpath = \"')]\"\n",
    "    rename_dict = {\"STATE\": \"Country\",\"CZ_NAME\":\"District\", \"TOR_F_SCALE\": \"Scale\", \"TOR_LENGTH\":\"Length\",\"TOR_WIDTH\":\"Width\",\"BEGIN_LOCATION\":\"City\",\"BEGIN_LAT\":\"Latitude\",\"BEGIN_LON\":\"Longtitude\", \"DEATHS_DIRECT\": \"Deaths\", \"YEAR\": \"Year\"}\n",
    "    col_list = [\"BEGIN_YEARMONTH\", \"BEGIN_DAY\", \"STATE\", \"EVENT_TYPE\", \"CZ_NAME\", \"BEGIN_DATE_TIME\", \"DEATHS_DIRECT\", \"TOR_F_SCALE\", \"TOR_LENGTH\", \"TOR_WIDTH\", \"BEGIN_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\"]\n",
    "    df_list = []\n",
    "    csv_link = \"https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "    csv_driver = create_web_driver()\n",
    "    start_web_driver(csv_driver, csv_link)\n",
    "    time.sleep(1)\n",
    "    for i in range(1994,2018):\n",
    "        df_to_append = pd.read_csv(csv_link + csv_driver.find_element_by_xpath(f\"{start_csv_xpath}{i}{end_csv_xpath}\").text, compression='gzip', usecols=col_list)\n",
    "        df_to_append.rename(columns=rename_dict, inplace=True)\n",
    "        split_df = df_to_append.BEGIN_DATE_TIME.str.split(' ', expand=True)\n",
    "        df_to_append = df_to_append[df_to_append['EVENT_TYPE'] == 'Tornado'].drop(columns=['EVENT_TYPE', \"BEGIN_DATE_TIME\"])\n",
    "        df_to_append['Time'] = split_df[1].str[:5]\n",
    "        df_to_append = create_date_in_df(df_to_append)\n",
    "        df_to_append[\"Length\"] = df_to_append.Length.apply(lambda x: convert_miles_to_km(x))\n",
    "        df_to_append[\"Width\"] = df_to_append.Width.apply(lambda x: convert_yards_to_meters(x))\n",
    "        df_list.append(df_to_append)\n",
    "    csv_df = pd.concat(df_list, ignore_index=True)\n",
    "    csv_df = csv_df[[\"District\",\"City\",\"Country\",\"Longtitude\", \"Latitude\" ,\"Date\",\"Time\",\"Scale\",\"Length\", \"Width\", \"Deaths\"]]\n",
    "    csv_df.to_csv('noaa_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable initialization for ESWD\n",
    "MONTHS_TO_COUNT = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "ESWD_URL = 'https://eswd.eu/cgi-bin/eswd.cgi'\n",
    "ESWD_TORNADO_XPATH = '//*[@name=\"TORNADO\"]'\n",
    "START_DATE_XPATH = '//*[@id=\"start_date\"]'\n",
    "END_DATE_XPATH = '//*[@id=\"end_date\"]'\n",
    "FIND_REPORTS_COUNT_XPATH = \"//p[contains(text(),'selected reports')] | //p[contains(text(),'no reports')]\"\n",
    "SUBMIT_XPATH = '(//*[@value=\"submit query\"])[2]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESWD auxilary functions for date parsing\n",
    "def get_end_date(curr_year, curr_month):\n",
    "    if curr_month == '02':\n",
    "        day = '28'\n",
    "    elif curr_month in ['04','06', '09','11']:\n",
    "        day = '30'\n",
    "    else:\n",
    "        day = '31'\n",
    "    return f'{day}-{curr_month}-{curr_year}'\n",
    "\n",
    "def get_start_date(curr_year,curr_month):\n",
    "    return f'01-{curr_month}-{curr_year}'\n",
    "\n",
    "def input_date(driver, start_date, end_date, start_date_xpath, end_date_xpath):\n",
    "    driver.find_element_by_xpath(start_date_xpath).clear()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(start_date_xpath).send_keys(start_date)\n",
    "    driver.find_element_by_xpath(end_date_xpath).clear()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath(end_date_xpath).send_keys(end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to accumulate the data from the site in lists to prepare for dataframe creation using other auxilary functions\n",
    "def get_data_from_page(driver,district, city, country, long, lat, date, times, scale,length, width):\n",
    "    base_xpath =\"//td[@class='base_info']/p[b and not(@class='smallgray')]\"\n",
    "    district_xpath = \"/b[1]\"\n",
    "    bold_tags_xpath = \"/b\"\n",
    "    country_xpath = \"/b[2]\"\n",
    "    date_xpath = \"/b[3]\"\n",
    "    time_xpath = \"/b[4]\"\n",
    "    scale_xpath_1 = \"(//p[@class='TORNADO detail_info_entry'])\"\n",
    "    scale_xpath_2 = \"/b[contains(text(),'F')]\" \n",
    "    scale_xpath_3 = \"((//p[@class='TORNADO'][contains(b,'tornado')])\"\n",
    "    scale_xpath_4 = \"/b[text()='tornado'])\"\n",
    "    reports_count = len(driver.find_elements_by_xpath(base_xpath))\n",
    "    len_elements = driver.find_elements_by_xpath(scale_xpath_1)\n",
    "    parent_items= driver.find_elements_by_xpath(base_xpath)\n",
    "    for i in range(reports_count):\n",
    "        district = get_elements_from_list(driver,district_xpath,base_xpath,district,i)\n",
    "        child_item = driver.find_elements_by_xpath((f'({base_xpath})[{i+1}]{bold_tags_xpath}'))\n",
    "        child_item_text = get_text_from_element_list(child_item)\n",
    "        city= get_city_list(child_item_text,parent_items[i].text,city,0)\n",
    "        country = get_elements_from_list(driver,country_xpath,base_xpath,country,i)\n",
    "        long, lat =  get_long_lat_lists(child_item_text,parent_items[i].text,long,lat,1)\n",
    "        date = get_elements_from_list(driver,date_xpath,base_xpath,date,i)\n",
    "        times = get_elements_from_list(driver,time_xpath,base_xpath,times,i)\n",
    "        scale_elements_list = driver.find_elements_by_xpath((f'{scale_xpath_1}[{i+1}]{scale_xpath_2} | {scale_xpath_3}[{i+1}]{scale_xpath_4}'))\n",
    "        scale = get_scale_list(scale,scale_elements_list)\n",
    "        length = get_len_list(child_item_text,parent_items[i].text,length,1,len_elements[i])\n",
    "        width = get_width_list(width, len_elements[i])\n",
    "    return district, city, country, long,lat, date, times, scale,length,width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions in charge of initial parsing and cleaning of the data captured by XPATH\n",
    "def replace_child_text(child_text,field):\n",
    "    for word in child_text.split(' '):\n",
    "        field = field.replace('\\n','').replace(word,'',1).lstrip()\n",
    "    return field\n",
    "\n",
    "def checks_split_position(field,selector):\n",
    "    if field[0] == '(':\n",
    "        selector = selector + 1\n",
    "    return selector\n",
    "\n",
    "def checks_digit(field,selector):\n",
    "    return field.split('(')[selector][0].isdigit()\n",
    "\n",
    "def checks_len(field):\n",
    "    return '<' in field\n",
    "    \n",
    "def get_text_from_element_list(elem_list):\n",
    "    text = ''\n",
    "    for elem in elem_list:\n",
    "        text += elem.text + ' '\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to clean the data further and appending it to the relevant lists\n",
    "def convert_w_e_to_long(longtitude):\n",
    "    if 'W' in longtitude:\n",
    "        longtitude = longtitude.replace('W','').strip()\n",
    "        longtitude = -float(longtitude)\n",
    "    elif 'E' in longtitude:\n",
    "        longtitude = longtitude.replace('E','').strip()\n",
    "        longtitude = float(longtitude)\n",
    "    return longtitude\n",
    "def convert_n_to_lat(latitude):\n",
    "    latitude = latitude.replace('N','').strip()\n",
    "    latitude = float(latitude)\n",
    "    return latitude\n",
    "\n",
    "def append_coordinates_lists(field,long,lat,position):\n",
    "    regexp = re.compile(r'[0-9]*\\.[0-9]+ [a-zA-Z]+')\n",
    "    if checks_len(field):\n",
    "        coordinates = field.split('(')[position].split(')')[0].rstrip()\n",
    "    else:\n",
    "        coordinates = field.split('(')[position].rstrip().replace(')','')\n",
    "    if not re.match(regexp,coordinates):\n",
    "        long.append(np.nan)\n",
    "        lat.append(np.nan)\n",
    "    else:\n",
    "        longtitude = coordinates.split(',')[1]\n",
    "        longtitude = convert_w_e_to_long(longtitude)\n",
    "        latitude = coordinates.split(',')[0]\n",
    "        latitude = convert_n_to_lat(latitude)\n",
    "        long.append(longtitude)\n",
    "        lat.append(latitude)\n",
    "    return long,lat\n",
    "\n",
    "def get_long_lat_lists(child_text,field,long,lat,selector):\n",
    "    field = replace_child_text(child_text,field)\n",
    "    position = checks_split_position(field, selector)\n",
    "    if checks_digit(field,position):\n",
    "            long,lat = append_coordinates_lists(field,long,lat,position)\n",
    "    else:\n",
    "        if checks_digit(field,position-1):\n",
    "            long,lat = append_coordinates_lists(field,long,lat,position-1)\n",
    "        else:\n",
    "            long.append(np.nan)\n",
    "            lat.append(np.nan)\n",
    "    return long,lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxilary functions to clean the data further and appending it to the relevant lists\n",
    "'''if the size of the elements list is 1 it gets only \"tornado\" dummy text, so there is no scale in the text, otherwise there is scale \n",
    "in the text so take the relevant F scale'''\n",
    "def get_scale_list(scale,scale_elements_list):\n",
    "    if len(scale_elements_list) == 1:\n",
    "        scale.append(np.nan)\n",
    "    else:\n",
    "        for element in scale_elements_list:\n",
    "            if 'F' in element.text:\n",
    "                scale.append(element.text)\n",
    "    return scale\n",
    "\n",
    "def get_width_list(curr_list, element):\n",
    "    if 'path width:' in element.text:\n",
    "        curr_list.append(element.text.split('path width:')[1].split('m')[0].strip()) \n",
    "    else:\n",
    "        curr_list.append(np.nan)\n",
    "    return curr_list\n",
    "\n",
    "'''if there's a field in the right textbox with specified path length get the number presented in it, otherwise check if the first char is a digit, if so check if the field on the left \n",
    "    has the \"<\" char, which indicates length, if so take it, otherwise input nan in the list'''\n",
    "def get_len_list(child_text,field,curr_list,selector,element):\n",
    "    if 'path length:' in element.text:\n",
    "        curr_list.append(element.text.split('path length:')[1].split('k')[0].strip()) \n",
    "    else:\n",
    "        field = replace_child_text(child_text,field)\n",
    "        if checks_digit(field,selector):\n",
    "            if checks_len(field):\n",
    "                curr_list.append(field.split('<')[1].split('k')[0].strip()) \n",
    "            else:\n",
    "                curr_list.append(np.nan)\n",
    "        else:\n",
    "            curr_list.append(np.nan)\n",
    "    return curr_list\n",
    "'''if the first character is a digit put nan, otherwise take the city name'''\n",
    "def get_city_list(child_text,field,curr_list,selector):\n",
    "    field = replace_child_text(child_text,field)\n",
    "    position = checks_split_position(field, selector)\n",
    "    if checks_digit(field,position):\n",
    "        curr_list.append(np.nan)\n",
    "    else:\n",
    "        curr_list.append(field.split('(')[position].rstrip().replace(')',''))\n",
    "    return curr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary functions to input bold text to the relevant lists and create a df using the lists\n",
    "def get_elements_from_list(driver,curr_xpath,base_xpath,curr_list,i):\n",
    "    curr_list.append(driver.find_elements_by_xpath((base_xpath+curr_xpath))[i].text)\n",
    "    return curr_list\n",
    "\n",
    "def get_df_from_lists(district, city, country, long, lat, date, times, scale,length,width):\n",
    "    df = pd.DataFrame({'District':district,'City':city,'Country':country,'Longtitude':long,'Latitude':lat, 'Date':date,'Time':times,'Scale':scale,'Length (KM)':length,'Width (M)':width})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to crawl the data from the website and create a df\n",
    "def run_eswd_crawler():\n",
    "    driver = create_web_driver()\n",
    "    start_web_driver(driver,ESWD_URL)\n",
    "    driver.find_element_by_xpath(ESWD_TORNADO_XPATH).click()\n",
    "    district, city, country, long, lat, date, times, scale,length, width = [], [], [], [], [], [], [], [],[],[]\n",
    "    for year in YEARS_TO_COUNT:\n",
    "        for month in MONTHS_TO_COUNT:\n",
    "            end_date = get_end_date(year,month)\n",
    "            start_date = get_start_date(year, month)\n",
    "            input_date(driver, start_date, end_date,START_DATE_XPATH,END_DATE_XPATH)\n",
    "            driver.find_element_by_xpath(SUBMIT_XPATH).click()\n",
    "            reports_amount = driver.find_element_by_xpath(FIND_REPORTS_COUNT_XPATH).text\n",
    "            log(f'{year}-{month}-{reports_amount}', 'amount_of_reports.log')\n",
    "            if \"no report\" in reports_amount:\n",
    "                continue\n",
    "            district, city, country, long, lat, date, times, scale,length,width = get_data_from_page(driver,district, city, country, long, lat, date, times, scale,length,width)\n",
    "            edf = get_df_from_lists(district, city, country, long, lat, date, times, scale,length,width)\n",
    "    edf.to_csv(f'ESWD_data.csv', index=False)\n",
    "    close_web_driver(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eswd_crawler()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
